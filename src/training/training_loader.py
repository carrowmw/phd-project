from typing import Tuple
import numpy as np
import torch

from torch.utils.data import Dataset, DataLoader

from src.training.training_datasets import TimeSeriesDataset
from src.models import model_definitions
from src.utils.general_utils import load_config


def sliding_windows(data: np.ndarray, **kwargs) -> Tuple:
    """
    Generate sliding windows from the provided time-series data for sequence learning.

    Parameters:
    - data (np.ndarray): The time-series data from which windows will be generated.
    - window_size (int): Specifies the size of each sliding window.
    - input_feature_indices (list of ints | None): The indices of features to be considered as
    input.
    - target_feature_index (int): Index of the feature that needs to be predicted.
    - horizon (int): How many steps ahead the prediction should be.
    - stride (int, optional): Steps between the start of each window. Defaults to 1.
    - shapes (bool, optional): If set to True, it prints shapes of input and target for the
    first window. Defaults to False.

    Returns:
    - tuple: Contains inputs and targets as torch tensors.
    """
    training_loader_config = load_config("configs/training_loader_config.json")
    input_feature_indices = training_loader_config["kwargs"]["input_feature_indices"]
    target_feature_index = training_loader_config["kwargs"]["target_feature_index"]
    window_size = kwargs.get("window_size")
    horizon = kwargs.get("horizon")
    stride = kwargs.get("stride")
    print_shapes = kwargs.get("print_shapes")
    # If input_feature_indices is None, generate indices from 1 to number of features in the array
    if input_feature_indices is None and target_feature_index == 0:
        input_feature_indices = list(range(0, data.shape[1]))
    elif target_feature_index != 0:
        print("Target feature not in the first column of array - check configs")
    print(f"Input feature indices: {input_feature_indices}")
    inputs = []
    targets = []
    for i in range(0, len(data) - window_size - horizon + 1, stride):
        input_data = data[i : i + window_size, input_feature_indices]
        target_data = data[i + window_size + horizon - 1, target_feature_index]
        if print_shapes == True and i == 1:
            print("Sample sliding window:")
            print(f"Input shape: {input_data.shape}")
            print(f"Target data: {target_data}")
        inputs.append(input_data)
        targets.append(target_data)
    print(f"Generated {len(inputs)} sliding windows")
    inputs = np.array(inputs)
    targets = np.array(targets)
    feature_dim = len(input_feature_indices)

    pipeline = (
        torch.tensor(inputs, dtype=torch.float32),
        torch.tensor(targets, dtype=torch.float32),
        feature_dim,
    )

    return pipeline


def prepare_dataloaders(
    pipeline: Tuple[torch.Tensor, torch.Tensor, int], **kwargs
) -> Tuple[DataLoader, DataLoader, int]:
    """
    Prepares training and test dataloaders using the inputs and targets generated by the sliding
    windows function.

    Parameters:
    - inputs (torch.Tensor): Inputs generated by the sliding windows function.
    - targets (torch.Tensor): Targets generated by the sliding windows function.
    - batch_size (int): Number of samples per batch to load.
    - shuffle (bool, optional): Whether to shuffle the data samples. Defaults to False.
    - num_workers (int, optional): Number of subprocesses to use for data loading. Defaults to 0.

    Returns:
    - Tuple[DataLoader, DataLoader]:
        Contains train DataLoader, test DataLoader
    """
    inputs, targets = pipeline[0], pipeline[1]
    feature_dim = pipeline[2]
    batch_size = kwargs.get("batch_size")
    num_workers = kwargs.get("num_workers")
    shuffle = kwargs.get("shuffle")

    # Calculate train/test split index
    train_size = int(0.8 * len(inputs))

    # Split data into train and test sets
    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]
    train_targets, test_targets = targets[:train_size], targets[train_size:]

    # Create custom PyTorch Dataset instances
    train_dataset = TimeSeriesDataset(train_inputs, train_targets)
    test_dataset = TimeSeriesDataset(test_inputs, test_targets)

    # Create DataLoader instances for train and test sets
    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers
    )

    pipeline = (train_dataloader, test_dataloader, feature_dim)
    return pipeline


def load_model_and_dataloaders(pipeline: Tuple[DataLoader, DataLoader, int], **kwargs):
    training_loader_config = load_config("configs/training_loader_config.json")
    # model_config = load_config("configs/model_config.json")

    feature_dim = pipeline[2]

    print(f"Feature dimensions: {feature_dim}")

    # Load model type and normalise to match class naming convention
    model_type = training_loader_config["kwargs"]["model_type"]
    if model_type.lower() == "lstm":
        model_type = "LSTM" + "Model"
    elif model_type.lower() == "randomforest":
        model_type = "RandomForest" + "Model"
    else:
        model_type = model_type.capitalize() + "Model"

    print(f"\n\nAttempting to load: {model_type}\n\n")

    # Dynamically load the model class from model_definitions module
    if model_type not in dir(model_definitions):
        raise ValueError(f"Model type {model_type} not found in model_definitions.py")

    ModelClass = getattr(model_definitions, model_type)

    # Initialize model with provided configuration
    model = ModelClass(feature_dim)
    print("Model loaded successfully.")

    pipeline = (model, pipeline[0], pipeline[1])

    return pipeline
