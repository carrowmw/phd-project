from typing import Tuple
import numpy as np
import torch

from torch.utils.data import Dataset, DataLoader

from training.training_datasets import TimeSeriesDataset


def sliding_windows(data: np.ndarray, **kwargs) -> Tuple:
    """
    Generate sliding windows from the provided time-series data for sequence learning.

    Parameters:
    - data (np.ndarray): The time-series data from which windows will be generated.
    - window_size (int): Specifies the size of each sliding window.
    - input_feature_indices (list of ints | None): The indices of features to be considered as input.
    - target_feature_index (int): Index of the feature that needs to be predicted.
    - horizon (int): How many steps ahead the prediction should be.
    - stride (int, optional): Steps between the start of each window. Defaults to 1.
    - shapes (bool, optional): If set to True, it prints shapes of input and target for the first window. Defaults to False.

    Returns:
    - tuple: Contains inputs and targets as torch tensors.
    """
    window_size = kwargs.get("window_size")
    horizon = kwargs.get("horizon")
    stride = kwargs.get("stride")
    input_feature_indices = kwargs.get("input_feature_indices")
    target_feature_index = kwargs.get("target_feature_index")
    print_shapes = kwargs.get("print_shapes")
    # If input_feature_indices is None, generate indices from 1 to number of features in the array
    if input_feature_indices is None and target_feature_index == 0:
        input_feature_indices = list(range(1, data.shape[1]))
    elif target_feature_index != 0:
        print("Target feature not in the first column of array - check configs")

    inputs = []
    targets = []
    for i in range(0, len(data) - window_size - horizon + 1, stride):
        input_data = data[i : i + window_size, input_feature_indices]
        target_data = data[i + window_size + horizon - 1, target_feature_index]
        if print_shapes == True and i == 1:
            print("Sampe sliding window:")
            print(f"Input shape: {input_data.shape}")
            print(f"Target data: {target_data}")
        inputs.append(input_data)
        targets.append(target_data)
    print(f"Generated {len(inputs)} sliding windows")
    inputs = np.array(inputs)
    targets = np.array(targets)

    input_and_target_tensors = (
        torch.tensor(inputs, dtype=torch.float32),
        torch.tensor(targets, dtype=torch.float32),
    )

    return input_and_target_tensors


def prepare_dataloaders(
    tensors: Tuple, **kwargs
) -> Tuple[Dataset, Dataset, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepares training and test dataloaders using the inputs and targets generated by the sliding windows function.

    Parameters:
    - inputs (torch.Tensor): Inputs generated by the sliding windows function.
    - targets (torch.Tensor): Targets generated by the sliding windows function.
    - batch_size (int): Number of samples per batch to load.
    - shuffle (bool, optional): Whether to shuffle the data samples. Defaults to False.
    - num_workers (int, optional): Number of subprocesses to use for data loading. Defaults to 0.

    Returns:
    - Tuple[DataLoader, DataLoader, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        Contains train DataLoader, test DataLoader, test inputs, test targets, train inputs, and train targets.
    """
    inputs, targets = tensors[0], tensors[1]

    batch_size = kwargs.get("batch_size")
    num_workers = kwargs.get("num_workers")
    shuffle = kwargs.get("shuffle")

    # Calculate train/test split index
    train_size = int(0.8 * len(inputs))

    # Split data into train and test sets
    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]
    train_targets, test_targets = targets[:train_size], targets[train_size:]

    # Create custom PyTorch Dataset instances
    train_dataset = TimeSeriesDataset(train_inputs, train_targets)
    test_dataset = TimeSeriesDataset(test_inputs, test_targets)

    # Create DataLoader instances for train and test sets
    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers
    )

    return (
        train_dataloader,
        test_dataloader,
        test_inputs,
        test_targets,
        train_inputs,
        train_targets,
    )
